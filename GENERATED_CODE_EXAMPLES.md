# Generated Code Examples

This document shows examples of the code generated by the `#[derive(ParquetSerialize)]` macro, particularly focusing on how collection types are handled.

## Simple Collection Example

For a struct like:

```rust
#[derive(ParquetSerialize)]
struct VecExample {
    id: u32,
    values: Vec<i32>,
}
```

The macro generates implementations similar to:

```rust
impl ParquetSerialize for VecExample {
    fn arrow_schema() -> Arc<Schema> {
        Arc::new(Schema::new(vec![
            Field::new("id", DataType::UInt32, false),
            Field::new(
                "values", 
                DataType::List(Arc::new(Field::new("item", DataType::Int32, false))), 
                false
            ),
        ]))
    }

    fn dump(items: Vec<VecExample>) -> Result<Vec<u8>, ParquetError> {
        let schema = Self::arrow_schema();
        let mut buf = Vec::new();
        let mut writer = ArrowWriter::try_new(&mut buf, schema.clone(), None)?;

        // Build arrays for each field
        let id_array = Arc::new(UInt32Array::from(
            items.iter().map(|item| item.id).collect::<Vec<_>>()
        )) as ArrayRef;

        // For Vec<i32>, build ListArray
        let mut offsets = vec![0i32];
        let mut values = Vec::new();
        let mut current_offset = 0i32;
        for item in &items {
            current_offset += item.values.len() as i32;
            offsets.push(current_offset);
            values.extend_from_slice(&item.values);
        }
        let value_array = Int32Array::from(values);
        let field = Arc::new(Field::new("item", DataType::Int32, false));
        let offsets = OffsetBuffer::new(offsets.into());
        let values_array = Arc::new(ListArray::new(field, offsets, Arc::new(value_array), None)) as ArrayRef;

        let columns = vec![id_array, values_array];
        let batch = RecordBatch::try_new(schema, columns)?;
        writer.write(&batch)?;
        writer.close()?;
        Ok(buf)
    }

    fn load(buf: &[u8]) -> Result<Vec<VecExample>, ParquetError> {
        let bytes = Bytes::copy_from_slice(buf);
        let builder = ParquetRecordBatchReaderBuilder::try_new(bytes)?;
        let reader = builder.build()?;

        let mut result = vec![];
        for maybe_batch in reader {
            let batch = maybe_batch?;
            
            // Extract id column
            let id_col = batch
                .column(0)
                .as_any()
                .downcast_ref::<UInt32Array>()
                .ok_or_else(|| ParquetError::General("id column type mismatch".to_string()))?;
            
            // Extract values column (ListArray)
            let values_col = batch
                .column(1)
                .as_any()
                .downcast_ref::<ListArray>()
                .ok_or_else(|| ParquetError::General("values column type mismatch".to_string()))?;
            
            let values_values = values_col.values();
            let values_i32 = values_values
                .as_any()
                .downcast_ref::<Int32Array>()
                .ok_or_else(|| ParquetError::General("values inner type mismatch".to_string()))?;

            for i in 0..batch.num_rows() {
                let id = id_col.value(i);
                
                // Extract Vec<i32> from ListArray
                let (start, end) = (
                    values_col.value_offsets()[i] as usize,
                    values_col.value_offsets()[i + 1] as usize
                );
                let vec_values = values_i32.values()[start..end].to_vec();
                
                result.push(VecExample {
                    id,
                    values: vec_values,
                });
            }
        }
        
        Ok(result)
    }
}
```

## HashSet Example

For a struct with HashSet:

```rust
#[derive(ParquetSerialize)]
struct HashSetExample {
    id: u32,
    unique_values: HashSet<String>,
}
```

The macro handles HashSet by converting it to a Vec internally and storing as a ListArray, with similar extraction logic but converting back to HashSet during deserialization.

## Key Benefits of Native Collection Support

1. **Better Performance**: Collections are stored as native Arrow LIST types instead of JSON strings
2. **Type Safety**: Element types are preserved and validated at the Arrow level
3. **Query Efficiency**: Parquet readers can efficiently access individual elements of collections
4. **Memory Efficiency**: No need to serialize/deserialize JSON strings

The generated code properly handles all supported collection element types (i8, i16, i32, i64, u8, u16, u32, u64, f32, f64, bool, String) with appropriate Arrow array types.